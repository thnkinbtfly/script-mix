# Script-mix: Mixing Scripts for Low-resource Language Parsing

This work is implemented based on [adapter-transformers](https://github.com/adapter-hub/adapter-transformers).

## LA training (at TPU VM)
```bash
pip install -r requirements.txt
pip install -r examples/pytorch/language-modeling/requirements.txt

export model_name=TOKENIZER_SPECIALIZED_MBERT
export unlabelled_data_dir=PATH_TO_UNLABELLED_DATA
export output_dir=OUTPUT_DIR
export XRT_TPU_CONFIG="localservice;0;localhost:51011"
export train_bs=8 
python examples/pytorch/xla_spawn.py --num_cores 8 examples/pytorch/language-modeling/run_mlm.py --train_file ${unlabelled_data_dir}/all.txt --max_steps 50000 --save_strategy steps --save_steps 50000 --max_seq_length 512      --model_name_or_path $model_name     --do_train     --per_device_train_batch_size $train_bs     --learning_rate 1e-4     --output_dir $output_dir     --save_total_limit 2     --pad_to_max_length        --train_adapter   --adapter_reduction_factor 2  --adapter_config "pfeiffer+inv"
```
To train Language module for UniPELT, change `--adapter_reduction_factor 2  --adapter_config "pfeiffer+inv"` into `--reduction_r 8  --adapter_config "unipelt+inv"`

## Training single LA with Dual-script
```bash
export va_data_dir=PATH_TO_VA_DATA
export tl_data_dir=PATH_TO_TL_DATA
export output_dir=OUTPUT_DIR
export XRT_TPU_CONFIG="localservice;0;localhost:51011"
export train_bs=1 # with grad_accum 2 and len_per_language 4, per_device_batch_size is 8 actually
python examples/pytorch/xla_spawn.py --num_cores 8 examples/pytorch/language-modeling/run_mlm.py --languages 0 1 --train_files ${va_data_dir}/all.txt ${tl_data_dir}/all.txt --len_per_language 4 --do_shuffle=False --max_steps 50000 --save_strategy steps --save_steps 50000 --max_seq_length 512      --model_name_or_path $model_name     --do_train     --per_device_train_batch_size $train_bs     --learning_rate 1e-4     --output_dir $output_dir    --save_total_limit 2     --pad_to_max_length     --gradient_accumulation_steps 2     --train_adapter     --adapter_reduction_factor 2     --adapter_config "pfeiffer+inv"
````

## fine-tuning MAD-X (at NGC 21.03 docker)
### DEP
```bash
export PYTHONPATH=`pwd`
export model_name=TOKENIZER_SPECIALIZED_MBERT
export output_dir=OUTPUT_DIR
export eval_lang=EVAL_LANG
export train_bs=16
export grad_accum=2
export fp16=True
export epochs=30
export lr=5e-4

export adapter_config=pfeiffer # or unipelt
export train_dir_suffix= # or 0~6 if cross-valid with full data, or fewshot0~6 if fewshot cross-valid
export data_size_suffix= # or _16 if 16-shot, _32 if 32-shot
export data_setting="--task_name dep --max_seq_length 256 --train_file datas/${eval_lang}/ud${train_dir_suffix}/train${data_size_suffix}.json --validation_file datas/${eval_lang}/ud${train_dir_suffix}/dev${data_size_suffix}.json --test_file datas/${eval_lang}/ud${train_dir_suffix}/test.json "
export adapter_setting="--train_adapter=True --adapter_config=${adapter_config} --load_lang_adapter ${lang_adapter_dir} --language ${eval_lang}"
export train_setting="--gradient_accumulation_steps ${grad_accum} --metric_score las --learning_rate $lr --fp16=$fp16 --evaluation_strategy=epoch --save_strategy=epoch --save_total_limit=2 --metric_for_best_model=las --greater_is_better=True --num_train_epochs=$epochs --per_device_train_batch_size $train_bs --model_name_or_path $model_name --output_dir ${output_dir}"

export lang_adapter_dir=PATH_TO_TRAINED_LANG_MODULE

python3 examples/pytorch/dependency-parsing/run_udp.py --do_train=True --do_predict=True --per_device_eval_batch_size=8 $adapter_setting $train_setting $data_setting
python3 examples/pytorch/dependency-parsing/run_udp.py --do_train=False --do_predict=True --load_best --per_device_eval_batch_size=8 $adapter_setting $train_setting $data_setting
```

### POS
```bash
export PYTHONPATH=`pwd`
export model_name=TOKENIZER_SPECIALIZED_MBERT
export output_dir=OUTPUT_DIR
export eval_lang=EVAL_LANG
export train_bs=16
export grad_accum=2
export fp16=True
export epochs=30
export lr=5e-4

export adapter_config=pfeiffer # or unipelt
export train_dir_suffix= # or 0~6 if cross-valid with full data, or fewshot0~6 if fewshot cross-valid
export data_size_suffix= # or _16 if 16-shot, _32 if 32-shot
export data_setting="--task_name pos --max_seq_length 256 --train_file datas/${eval_lang}/ud${train_dir_suffix}/train${data_size_suffix}.conllu --validation_file datas/${eval_lang}/ud${train_dir_suffix}/dev${data_size_suffix}.conllu --test_file datas/${eval_lang}/ud${train_dir_suffix}/test.conllu "
export train_setting="--gradient_accumulation_steps ${grad_accum} --overwrite_cache --learning_rate $lr --fp16=$fp16 --evaluation_strategy=epoch --save_strategy=epoch --save_total_limit=2 --metric_for_best_model=accuracy --greater_is_better=True --num_train_epochs=$epochs --per_device_train_batch_size $train_bs --model_name_or_path $model_name --output_dir $output_dir "

export lang_adapter_dir=PATH_TO_TRAINED_LANG_MODULE
export adapter_setting="--train_adapter=True --adapter_config=${adapter_config} --load_lang_adapter ${lang_adapter_dir} --language ${eval_lang}"

python3 examples/pytorch/token-classification/run_pos.py --do_train=True --do_predict=True --per_device_eval_batch_size=8 $adapter_setting $train_setting $data_setting
python3 examples/pytorch/token-classification/run_pos.py --do_train=False --do_predict=True --load_best --per_device_eval_batch_size=8 $adapter_setting $train_setting $data_setting
```

## fine-tuning with script-mix
change the last lines of the MAD-X fine-tuning script as follows:
```bash
export lang_adapter="PATH_TO_VA_TRAINED_LANG_MODULE PATH_TO_TL_TRAINED_LANG_MODULE "
export fuse_mode="--fuse_mode lang_fuse_train_fuse_and_task " # add "--mlp_fusion_mode=key" to use AdapterFusion+/UniPELTFusion+
export adapter_setting="${fuse_mode} --train_adapter=True --adapter_config=${adapter_config}  --load_lang_adapter ${lang_adapter_dir} --language VA_LANG TL_LANG "

python3 examples/pytorch/dependency-parsing/run_udp_fusion.py --do_train=True --do_predict=True --per_device_eval_batch_size=8 $task_setting $train_setting $data_setting
python3 examples/pytorch/dependency-parsing/run_udp_fusion.py --do_train=False --do_predict=True --load_best --per_device_eval_batch_size=8 $task_setting $train_setting $data_setting
```
